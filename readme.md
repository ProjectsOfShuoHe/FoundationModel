**Paper list for Robustness and Security in Foundation Models:**

Foundation models include Large language models (LLMs), vision-language models (VLMs), and large vision models (LVMs). We concern about robustness and security (other related trustworthiness topics) in these foundation models.

**Survey:**

**[1]** DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. Arxiv. 2023. [http://arxiv.org/abs/2306.11698]

**[2]** PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. Arxiv. 2023. [http://arxiv.org/abs/2306.04528]

**[3]** Challenges and Applications of Large Language Models. Arxiv. 2023. [http://arxiv.org/abs/2307.10169]

**[4]** A Survey on Evaluation of Large Language Models. Arxiv. 2023. [http://arxiv.org/abs/2307.03109]

**Robustness in LLMs:**

**[1]** Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. Arxiv. 2023. [http://arxiv.org/abs/2306.04618]

**[2]** Jailbroken: How Does LLM Safety Training Fail? Arxiv. 2023. [http://arxiv.org/abs/2307.02483]

**[3]** Are aligned neural networks adversarially aligned? Arxiv. 2023. [https://arxiv.org/pdf/2306.15447.pdf]

**[4]** Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. Arxiv. 2023. [https://arxiv.org/pdf/2306.13063.pdf]

**Toxicity in LLMs:**

**[1]** RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. Arxiv. 2023. [https://aclanthology.org/2020.findings-emnlp.301]

**Backdoor in LLMs:**

**[1]** BadPrompt: Backdoor Attacks on Continuous Prompts. NeurIPS. 2022. [https://arxiv.org/abs/2211.14719]

**[2]** BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT. Arxiv. 2023. [http://arxiv.org/abs/2304.12298]

**[3]** Exploring the Universal Vulnerability of Prompt-based Learning Paradigm. NAACL. 2022. [https://aclanthology.org/2022.findings-naacl.137]

**[4]** Backdoor Pre-trained Models Can Transfer to All. CCS. 2021. [http://arxiv.org/abs/2111.00197]

**[5]** On the Exploitability of Instruction Tuning. Arxiv. 2023. [http://arxiv.org/abs/2306.17194]

**[6]** Poisoning Language Models During Instruction Tuning. ICML. 2023. [https://arxiv.org/abs/2305.00944]

**[7]** Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. Arxiv. 2023. [https://arxiv.org/abs/2305.14710]

**Robustness in VLMs:**

**[1]** On Evaluating Adversarial Robustness of Large Vision-Language Models. Arxiv. 2023. [http://arxiv.org/abs/2305.16934]

**[2]** Towards Robust Prompts on Vision-Language Models. Arxiv. 2023. [http://arxiv.org/abs/2304.08479]

**[3]** Debiased Fine-Tuning for Vision-language Models by Prompt Regularization. AAAI. 2023. [http://arxiv.org/abs/2301.12429]

**[4]** UNDERSTANDING ZERO-SHOT ADVERSARIAL ROBUSTNESS FOR LARGE-SCALE MODELS. ICLR. 2023. [http://arxiv.org/abs/2212.07016]

**[5]** Raising the Cost of Malicious AI-Powered Image Editing. Arxiv. 2023. [http://arxiv.org/abs/2302.06588]
 
**Backdoor in VLMs:**

**[1]** Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization. Arxiv. 2023. [http://arxiv.org/abs/2305.10701]

**[2]** CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. ICCV. 2023. [http://arxiv.org/abs/2303.03323]

**[3]** Backdoor Attacks to Pre-trained Unified Foundation Models. Arxiv. 2023. [http://arxiv.org/abs/2302.09360]

**[4]** Dual-Key Multimodal Backdoors for Visual Question Answering. CVPR. 2022. [https://openaccess.thecvf.com/content/CVPR2022/papers/Walmer_Dual-Key_Multimodal_Backdoors_for_Visual_Question_Answering_CVPR_2022_paper.pdf]

**[5]** POISONING AND BACKDOORING CONTRASTIVE LEARNING. ICLR. 2022. [https://openreview.net/pdf?id=iC4UHbQ01Mp]

**[6]** How to Backdoor Diffusion Models?. CVPR. 2023. [https://arxiv.org/abs/2212.05400]


**Robustness in LVMs:**


**Visual Tuning:**

**[1]** Visual Tuning. Arxiv. 2023. [http://arxiv.org/abs/2305.06061] 

**[2]** Review of Large Vision Models and Visual Prompt Engineering. Arxiv. 2023. [http://arxiv.org/abs/2307.00855]

**[3]** Visual Prompt Tuning. ECCV. 2022. [http://arxiv.org/abs/2203.12119]

**Backdoor in LVMs:**

**[1]** Backdoor Attacks on Self-Supervised Learning. CVPR. 2022. [https://ieeexplore.ieee.org/document/9879958/]

**[2]** Backdoor Attacks in the Supply Chain of Masked Image Modeling. Arxiv. 2022. [https://arxiv.org/abs/2210.01632]



